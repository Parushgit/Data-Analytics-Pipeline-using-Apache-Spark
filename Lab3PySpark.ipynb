{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LR on main data  = 72.2917\n",
      "Accuracy NB on main data  = 77.1068\n",
      "Confusion Matrix for main data\n",
      "[[ 9.  0.  1.  0.]\n",
      " [ 0.  8.  1.  1.]\n",
      " [ 0.  1.  9.  0.]\n",
      " [ 0.  2.  3.  5.]]\n",
      "Accuracy LR on unknown data  = 82.1212\n",
      "Accuracy NB on unknown data = 82.7871\n",
      "Confusion Matrix for unknown data\n",
      "[[ 9.  0.  1.  0.]\n",
      " [ 0.  7.  3.  0.]\n",
      " [ 0.  1.  8.  1.]\n",
      " [ 0.  1.  0.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------Lab3---------------------------------------------\n",
    "# The data has been trained using two models which are Logistic Regression and Naive Bayes.\n",
    "# Some code snippets have been taken from link: https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, NGram\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from operator import add\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# This is for the main data which has 200 files (Includes traning and test data)\n",
    "def part2(context):\n",
    "    file = context[0]\n",
    "    words = re.sub('[^a-z0-9]+',' ',context[1].lower()).split()\n",
    "    file = file.split(\"/\")[-1]\n",
    "    if(re.match( r'FileSportsRaw.*', file)):\n",
    "        return (0.0, file ,words)\n",
    "    elif(re.match( r'FilePoliticsRaw.*', file)):\n",
    "        return (1.0, file, words)\n",
    "    elif(re.match( r'FileBusinessRaw.*', file)):\n",
    "        return (2.0, file, words)\n",
    "    else:\n",
    "        return (3.0, file, words)\n",
    "\n",
    "# This is for the unknown data which has 40 files (Includes test data)\n",
    "def part2_unknown(context):\n",
    "    file = context[0]\n",
    "    words = re.sub('[^a-z0-9]+',' ',context[1].lower()).split()\n",
    "    file = file.split(\"/\")[-1]\n",
    "    if(re.match( r'FileSportsUnknown.*', file)):\n",
    "        return (0.0, file ,words)\n",
    "    elif(re.match( r'FilePoliticsUnknown.*', file)):\n",
    "        return (1.0, file, words)\n",
    "    elif(re.match( r'FileBusinessUnknown.*', file)):\n",
    "        return (2.0, file, words)\n",
    "    else:\n",
    "        return (3.0, file, words)\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName( \"part1\" )\n",
    "conf.set(\"spark.executor.memory\", \"2g\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "# -----------------------------------For main data set-----------------------------------------------\n",
    "#reading input\n",
    "data_lines =sc.wholeTextFiles(\"AllArticlesFolder\")\n",
    "\n",
    "#configuring SparkSession\n",
    "spark=SparkSession(sc)\n",
    "\n",
    "#Converting to datafram\n",
    "hasattr(data_lines, \"toDF\")\n",
    "\n",
    "#tokeinizing the words and converting into dataframes\n",
    "tokenizeDf = data_lines.map(part2).toDF([\"label\", \"fileName\", \"words\"])\n",
    "\n",
    "#removing the Stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_column\")\n",
    "filteredWordsDf = remover.transform(tokenizeDf)\n",
    "\n",
    "#finding the tf value\n",
    "hashingTF = HashingTF(inputCol = \"filtered_column\", outputCol = \"hashFeatures\")\n",
    "tf = hashingTF.transform(filteredWordsDf)\n",
    "\n",
    "#finding the idf value\n",
    "idf = IDF(inputCol = \"hashFeatures\", outputCol = \"features\", )\n",
    "idfModel = idf.fit(tf)\n",
    "modifiedData = idfModel.transform(tf)\n",
    "\n",
    "category0Data = modifiedData.filter(modifiedData.label == 0.0)\n",
    "category1Data = modifiedData.filter(modifiedData.label == 1.0)\n",
    "category2Data = modifiedData.filter(modifiedData.label == 2.0)\n",
    "category3Data = modifiedData.filter(modifiedData.label == 3.0)\n",
    "\n",
    "idxDf = category0Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "trainingData0 = idxDf.sort(\"idx\").limit(40)\n",
    "testData0 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "idxDf = category1Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "trainingData1 = idxDf.sort(\"idx\").limit(40)\n",
    "testData1 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "idxDf = category2Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "trainingData2 = idxDf.sort(\"idx\").limit(40)\n",
    "testData2 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "idxDf = category3Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "trainingData3 = idxDf.sort(\"idx\").limit(40)\n",
    "testData3 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "result_training_data_1 = trainingData0.union(trainingData1)\n",
    "result_training_data_2 = trainingData2.union(trainingData3)\n",
    "result_training_data = result_training_data_1.union(result_training_data_2)\n",
    "\n",
    "result_test_data_1 = testData0.union(testData1)\n",
    "result_test_data_2 = testData2.union(testData3)\n",
    "result_test_data = result_test_data_1.union(result_test_data_2)\n",
    "\n",
    "#--------Logistic Regression Model----------\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(result_training_data)\n",
    "\n",
    "predictions = lrModel.transform(result_test_data)\n",
    "# predictions.select(\"fileName\",\"probability\",\"label\",\"prediction\").rdd.saveAsTextFile(\"output_lr\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy LR on main data  = %g\" % (100 * accuracy))\n",
    "\n",
    "#--------Naive Bayes Classficiation Model----------\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "nbmodel = nb.fit(result_training_data)\n",
    "\n",
    "predictions = nbmodel.transform(result_test_data)\n",
    "predictions.filter(predictions['prediction'] == 0)\n",
    "# predictions.select(\"fileName\",\"probability\",\"label\",\"prediction\").rdd.saveAsTextFile(\"output_nb\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy NB on main data  = %g\" % (100 * accuracy))\n",
    "print(\"Confusion Matrix for main data\")\n",
    "# Building dataset of prediction and label for Confusion Matrix\n",
    "\n",
    "predictionsLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "metrics = MulticlassMetrics(predictionsLabels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "print(cm)\n",
    "\n",
    "# -----------------------------------For unknown data set-----------------------------------------------\n",
    "#reading input\n",
    "data_lines =sc.wholeTextFiles(\"UnknownArticlesFolder\")\n",
    "\n",
    "hasattr(data_lines, \"toDF\")\n",
    "\n",
    "#tokeinizing the words and converting into dataframes\n",
    "tokenizeDf = data_lines.map(part2_unknown).toDF([\"label\", \"fileName\", \"words\"])\n",
    "\n",
    "#removing the Stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_column\")\n",
    "filteredWordsDf = remover.transform(tokenizeDf)\n",
    "\n",
    "#finding the tf value\n",
    "hashingTF = HashingTF(inputCol = \"filtered_column\", outputCol = \"hashFeatures\")\n",
    "tf = hashingTF.transform(filteredWordsDf)\n",
    "\n",
    "#finding the idf value\n",
    "idf = IDF(inputCol = \"hashFeatures\", outputCol = \"features\", )\n",
    "idfModel = idf.fit(tf)\n",
    "modifiedData = idfModel.transform(tf)\n",
    "\n",
    "category0Data = modifiedData.filter(modifiedData.label == 0.0)\n",
    "category1Data = modifiedData.filter(modifiedData.label == 1.0)\n",
    "category2Data = modifiedData.filter(modifiedData.label == 2.0)\n",
    "category3Data = modifiedData.filter(modifiedData.label == 3.0)\n",
    "\n",
    "idxDf = category0Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "unknown_testData0 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "idxDf = category1Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "unknown_testData1 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "idxDf = category2Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "unknown_testData2 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "idxDf = category3Data.withColumn(\"idx\", monotonically_increasing_id())\n",
    "unknown_testData3 = idxDf.sort(\"idx\", ascending = False).limit(10)\n",
    "\n",
    "unknown_test_data_1 = unknown_testData0.union(unknown_testData1)\n",
    "unknown_test_data_2 = unknown_testData2.union(unknown_testData3)\n",
    "unknown_test_data = unknown_test_data_1.union(unknown_test_data_2)\n",
    "\n",
    "#--------Logistic Regression Model----------\n",
    "predictions = lrModel.transform(unknown_test_data)\n",
    "# predictions.select(\"fileName\",\"probability\",\"label\",\"prediction\").rdd.saveAsTextFile(\"sparkFolder/output\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy LR on unknown data  = %g\" % (100 * accuracy))\n",
    "\n",
    "#--------Naive Bayes Classficiation Model----------\n",
    "predictions = nbmodel.transform(unknown_test_data)\n",
    "predictions.filter(predictions['prediction'] == 0)\n",
    "#predictions.select(\"fileName\",\"probability\",\"label\",\"prediction\").rdd.saveAsTextFile(\"sparkFolder/outputNB\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy NB on unknown data = %g\" % (100 * accuracy))\n",
    "print(\"Confusion Matrix for unknown data\")\n",
    "\n",
    "# Building dataset of prediction and label for Confusion Matrix\n",
    "predictionsLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "metrics = MulticlassMetrics(predictionsLabels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "print(cm)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
