{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------Lab3_TitanicData.ipynb------------------------------------------\n",
    "# Source - https://6chaoran.wordpress.com/2016/08/13/__trashed/\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# sc = SparkContext(\"local\", \"titanic\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Load csv file as RDD\n",
    "train_rdd = sc.textFile(\"train.csv\")\n",
    "test_rdd = sc.textFile(\"test.csv\")\n",
    "\n",
    "\n",
    "# Parse RDD to DF\n",
    "def parseTrain(rdd):\n",
    "    # extract data header (first row)\n",
    "    header = rdd.first()\n",
    "    # remove header\n",
    "    body = rdd.filter(lambda r: r!=header)\n",
    "\n",
    "    def parseRow(row):\n",
    "        # a function to parse each text row into\n",
    "        # data format\n",
    "        \n",
    "        # remove double quote, split the text row by comma\n",
    "        row_list = row.replace('\"','').split(\",\")\n",
    "        # convert python list to tuple, which is \n",
    "        # compatible with pyspark data structure\n",
    "        row_tuple = tuple(row_list)\n",
    "        return row_tuple\n",
    "\n",
    "    rdd_parsed = body.map(parseRow)\n",
    "    \n",
    "    colnames = header.split(\",\")\n",
    "    colnames.insert(3,'FirstName')\n",
    "    \n",
    "    return rdd_parsed.toDF(colnames)\n",
    "\n",
    "def parseTest(rdd):\n",
    "    header = rdd.first()\n",
    "    body = rdd.filter(lambda r: r!=header)\n",
    "    \n",
    "    def parseRow(row):\n",
    "        row_list = row.replace('\"','').split(\",\")\n",
    "        row_tuple = tuple(row_list)\n",
    "        return row_tuple\n",
    "    \n",
    "    rdd_parsed = body.map(parseRow)\n",
    "\n",
    "    colnames = header.split(\",\")\n",
    "    colnames.insert(2,'FirstName')\n",
    "\n",
    "    return rdd_parsed.toDF(colnames)\n",
    "\n",
    "train_df = parseTrain(train_rdd)\n",
    "test_df = parseTest(test_rdd)\n",
    "\n",
    "\n",
    "## Add Survived column to test\n",
    "## And append train/test data\n",
    "\n",
    "train_df = train_df.withColumn('Mark',lit('train'))\n",
    "test_df = (test_df.withColumn('Survived',lit(0)).withColumn('Mark',lit('test')))\n",
    "\n",
    "test_df = test_df[train_df.columns]\n",
    "df = train_df.unionAll(test_df)\n",
    "\n",
    "## Data Cleaning/Manipulation\n",
    "## Convert Age, SibSp, Parch, Fare to Numeric\n",
    "df = (df.withColumn('Age',df['Age'].cast(\"double\")).withColumn('SibSp',df['SibSp'].cast(\"double\")).withColumn('Parch',df['Parch'].cast(\"double\")).withColumn('Fare',df['Fare'].cast(\"double\")).withColumn('Survived',df['Survived'].cast(\"double\")))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "## Impute missing Age and Fare\n",
    "numVars = ['Survived','Age','SibSp','Parch','Fare']\n",
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "missing = {var: countNull(df,var) for var in numVars}\n",
    "age_mean = df.groupBy().mean('Age').first()[0]\n",
    "fare_mean = df.groupBy().mean('Fare').first()[0]\n",
    "df = df.na.fill({'Age':age_mean,'Fare':fare_mean})\n",
    "\n",
    "\n",
    "# Feature Enginnering\n",
    "## 1. Extract Title from Name\n",
    "\n",
    "## created user defined function to extract title\n",
    "getTitle = udf(lambda name: name.split('.')[0].strip(),StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    "\n",
    "## 2. Index categorical variable\n",
    "catVars = ['Pclass','Sex','Embarked','Title']\n",
    "\n",
    "## index Sex variable\n",
    "\n",
    "si = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\n",
    "df_indexed = si.fit(df).transform(df).drop('Sex').withColumnRenamed('Sex_indexed','Sex')\n",
    "\n",
    "## make use of pipeline to index all categorical variables\n",
    "def indexer(df,col):\n",
    "    si = StringIndexer(inputCol = col, outputCol = col+'_indexed').fit(df)\n",
    "    return si\n",
    "\n",
    "indexers = [indexer(df,col) for col in catVars]\n",
    "\n",
    "pipeline = Pipeline(stages = indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "\n",
    "## 3. Convert to label/features format\n",
    "catVarsIndexed = [i+'_indexed' for i in catVars]\n",
    "featuresCol = numVars+catVarsIndexed\n",
    "featuresCol.remove('Survived')\n",
    "labelCol = ['Mark','Survived']\n",
    "\n",
    "row = Row('mark','label','features')\n",
    "\n",
    "df_indexed = df_indexed[labelCol+featuresCol]\n",
    "# 0-mark, 1-label, 2-features\n",
    "lf = df_indexed.rdd.map(lambda r: (row(r[0], r[1],DenseVector(r[2:])))).toDF()\n",
    "# index label \n",
    "lf = StringIndexer(inputCol = 'label',outputCol='index').fit(lf).transform(lf)\n",
    "\n",
    "# split back train/test data\n",
    "train = lf.where(lf.mark =='train')\n",
    "test = lf.where(lf.mark =='test')\n",
    "\n",
    "# random split further to get train/validate\n",
    "train,validate = train.randomSplit([0.7,0.3],seed =121)\n",
    "\n",
    "print('Train Data Number of Row: '+ str(train.count()))\n",
    "print('Validate Data Number of Row: '+ str(validate.count()))\n",
    "print('Test Data Number of Row: '+ str(test.count()))\n",
    "\n",
    "# Apply Logsitic Regression\n",
    "\n",
    "# regPara: regualrization parameter\n",
    "lr = LogisticRegression(maxIter = 100, regParam = 0.05, labelCol='index').fit(train)\n",
    "\n",
    "# Evaluate model based on auc ROC(default for binary classification)\n",
    "\n",
    "\n",
    "def testModel(model, validate = validate):\n",
    "    pred = model.transform(validate)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'index')\n",
    "    return evaluator.evaluate(pred)\n",
    "\n",
    "print(str(testModel(lr)))\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth = 3, labelCol ='index').fit(train)\n",
    "rf = RandomForestClassifier(numTrees = 100, labelCol = 'index').fit(train)\n",
    "\n",
    "\n",
    "models = {'LogisticRegression':lr, 'DecistionTree':dt, 'RandomForest':rf}\n",
    "\n",
    "modelPerf = {k:testModel(v) for k,v in models.items()}\n",
    "print(modelPerf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
